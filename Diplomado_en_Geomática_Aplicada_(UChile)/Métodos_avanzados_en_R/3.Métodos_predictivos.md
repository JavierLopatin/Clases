## 5. Modelos predictivos con _Machine Learning_ y caret

<p align="right">
  <img width="300" height="300" src="img/caret_R.jpg">
</p>

Durante los últimos años, el interés y la aplicación de _machine learning_ ha experimentado tal expansión, que se ha convertido en una disciplina aplicada en prácticamente todos los ámbitos de investigación académica e industrial. El creciente número de personas dedicadas a esta disciplina ha dado como resultado todo un repertorio de herramientas con las que, perfiles con especialización media, consiguen acceder a métodos predictivos potentes. El lenguaje de programación **R** es un ejemplo de ello.

El término _machine learning_ engloba al conjunto de algoritmos que permiten identificar patrones presentes en los datos y crear con ellos estructuras (modelos) que los representan. Una vez que los modelos han sido generados, se pueden emplear para predecir información sobre hechos o eventos que todavía no se han observado. Es importante recordar que, los sistemas de _machine learning_, solo son capaces de memorizar patrones que estén presentes en los datos con los que se entrenan, por lo tanto, solo pueden reconocer lo que han visto antes. Al emplear sistemas entrenados con datos pasados para predecir futuros se está asumiendo que, en el futuro, el comportamiento será el mismo, cosa que no siempre ocurre.

Dentro de los modelos predictivos, vamos a separar estos en dos grupos de acuerdo a si buscan predecir (1) valores continuos de una variable, o (2) valores discretos o clases de una variable. Estos se denominan modelos de **regresión** y **clasificación**, respectivamente.


### 5.1. Paquete caret

Hay muchas funciones diferentes para modelar datos espaciales en **R**. En muchos casos, cada paquete o función tiene una manera distinta de ingresar o analizar los datos. Por esta razón, en este módulo utilizaremos el paquete [``caret``](https://topepo.github.io/caret/), el cual es una interfaz que unifica bajo un único marco cientos de funciones de distintos paquetes (lista [aquí](https://topepo.github.io/caret/available-models.html)), facilitando en gran medida todas las etapas de de preprocesamiento, entrenamiento, optimización y validación de modelos predictivos. Existen otros proyectos similares y muy prometedores como ``mlr``, pero nosotros revisaremos unicamente ``caret`` en esta oportunidad. Para instalar ``caret`` utiliza la herramienta de RStudio o el siguiente código:

```R
# Instalación de los paquetes que unifica caret. Esta instalación puede tardar.
# Solo es necesario ejecutarla si no se dispone de los paquetes.
install.packages("caret", dependencies = c("Depends", "Suggests"))
```

### 5.2. Algoritmos

En este módulo, vamos a ver en detalle cuatro algoritmos distintos: _Partial Least Square_ (_PLS_), _Support Vector Machines_ (_SVM_), Redes Neuronales o _Neural Networks_ (_NN_), y _Random Forest_ (_RF_). Estos funcionan de manera distinta entre si, y pueden ser agrupados de manera distinta:

A. Tipo de algoritmo
- Modelos lineales: _PLS_
- Modelos no-lineal: _SVM_, _RF_, _NN_

B. Tipo de reducción de dimensionalidad
- Modelos con selección de variables (_Feature Selection_): son aquellos modelos que reducen dimensionalidad de los datos seleccionando predictores, sin alterar su información; _SVM_, _RF_, _NN_
- Modelos de extracción de variables (_Feature Extraction_): son aquellos modelos que reducen dimensionalidad de los datos mediante métodos de ordenación, es deir transforman la información original de los predictores; _PLS_.

Glosario de términos a usar:
- Hiperparámetro (_hiperparameters_)
- Selección de variables (_variable selection_)
- Importancia de variables (_variable importance_)
- Sobreajuste (_overfitting_)
- Colinealidad (_colineality_)
- Residuos (_residuals_)
- Autocorrelación espacial (_spatial autocorrelation_)

#### 5.2.1. _Partial Least Square_ (_PLS_)

El método _Partial Least Squares_ (_PLS_) crea componentes lineales de las variables predictivas mediante _ordination_. Funciona de manera similar a _PCA_ en su método de reducción, pero utiliza la información de la variable respuesta (_Y_) para maximizar lo mas posible la varianza de los predictores (_X_). La diferencia reside en que, mientras _PCA_ ignora la variable respuesta _Y_ para determinar las combinaciones lineales, PLS busca aquellas que, además de explicar la varianza observada de _X_, predicen _Y_ lo mejor posible. Puede considerarse como una versión supervisada de _PCA_ para predicción. Por lo mismo, en general _PLS_ guarda mejor información para predicción que _PCA_ en pocos componentes.

_PLS_ es utilizado enormemente en estudios con datos de teledetección ya que reduce eficientemente la información con altos grados de colinealidad (i.e., _colineality_; comparten mucha información similar entre si), como por ejemplo las bandas espectrales.  Antes de utilizar _PLS_, los predictores deben ser normalizados, especialmente las variables están en unidades distintas Similar a _PCA_. Si no se hece esto, _PLS_ podría dar mas importancia simplemente a variables con mayor varianza. Por lo tanto, los predictores deben ser adecuadamente preprocesados antes de realizar el _PLS_.

Al igual que mucho métodos lineales, PLS esta sujeto a problemas de sobreajuste (_overfitting₎ si muchas variables son incliidas durante el modelamiento.

**Paquetes y funciones**
La funcion básica de _PLS_ utiliza la función  ``plsr`` del paquete ``pls``.

**Hiperparámetros**:
- ``ncomp``: número de componentes a incluir.

**Importancia de las variables en _caret_**: Como _PLS_ hace componentes lineales de todos los predictores,  la importancia individual de cada variable va a depender del número de componentes que se seleccione. Al igual que en _PCA_, en _PLS_ se pueden obtener los pesos (_weights₎ de cada variable en cada componente. Luego, la importancia de cada variable sería una suma ponderada de los pesos de cada variable.

<p align="center">
  <img width="600" src="img/pls.png">
</p>


#### 5.2.2. _Support Vector Machines_ (_SVM_)

**Paquetes y funciones**
Existen muchos topod de _SVM_, segun el _kernel_ que se utilice. A modlo de ejemplo, vamos a utilizar el método con _kernel_ radial, el cual es uno de los mas utilizados en teledetección. El método ``svmRadial`` de ``caret`` emplea la función ``ksvm()`` del paquete ``kernlab``.

**Hiperparámetros**:
- ``sigma``: coeficiente del kernel radial.
- ``C``: penalización por violaciones del margen del hiperplano.

<p align="center">
  <img width="600" src="img/svm.png">
</p>


## 5. Validación

| Box 1 |
\|-\|
|blabla|
