## 5. Modelos predictivos con _Machine Learning_ y caret

<p align="right">
  <img width="300" height="300" src="img/caret_R.jpg">
</p>

Durante los últimos años, el interés y la aplicación de _machine learning_ ha experimentado tal expansión, que se ha convertido en una disciplina aplicada en prácticamente todos los ámbitos de investigación académica e industrial. El creciente número de personas dedicadas a esta disciplina ha dado como resultado todo un repertorio de herramientas con las que, perfiles con especialización media, consiguen acceder a métodos predictivos potentes. El lenguaje de programación **R** es un ejemplo de ello.

El término _machine learning_ engloba al conjunto de algoritmos que permiten identificar patrones presentes en los datos y crear con ellos estructuras (modelos) que los representan. Una vez que los modelos han sido generados, se pueden emplear para predecir información sobre hechos o eventos que todavía no se han observado. Es importante recordar que, los sistemas de _machine learning_, solo son capaces de memorizar patrones que estén presentes en los datos con los que se entrenan, por lo tanto, solo pueden reconocer lo que han visto antes. Al emplear sistemas entrenados con datos pasados para predecir futuros se está asumiendo que, en el futuro, el comportamiento será el mismo, cosa que no siempre ocurre.

Dentro de los modelos predictivos, vamos a separar estos en dos grupos de acuerdo a si buscan predecir (1) valores continuos de una variable, o (2) valores discretos o clases de una variable. Estos se denominan modelos de **regresión** y **clasificación**, respectivamente.


### 5.1. Paquete caret

Hay muchas funciones diferentes para modelar datos espaciales en **R**. En muchos casos, cada paquete o función tiene una manera distinta de ingresar o analizar los datos. Por esta razón, en este módulo utilizaremos el paquete [``caret``](https://topepo.github.io/caret/), el cual es una interfaz que unifica bajo un único marco cientos de funciones de distintos paquetes (lista [aquí](https://topepo.github.io/caret/available-models.html)), facilitando en gran medida todas las etapas de de preprocesamiento, entrenamiento, optimización y validación de modelos predictivos. Existen otros proyectos similares y muy prometedores como ``mlr``, pero nosotros revisaremos unicamente ``caret`` en esta oportunidad. Para instalar ``caret`` utiliza la herramienta de RStudio o el siguiente código:

```R
# Instalación de los paquetes que unifica caret. Esta instalación puede tardar.
# Solo es necesario ejecutarla si no se dispone de los paquetes.
install.packages("caret", dependencies = c("Depends", "Suggests"))
```

### 5.2. Algoritmos

En este módulo, vamos a ver en detalle cuatro algoritmos distintos: _Partial Least Square_ (_PLS_), _Support Vector Machines_ (_SVM_), Redes Neuronales o _Neural Networks_ (_NN_), y _Random Forest_ (_RF_). Estos funcionan de manera distinta entre si, y pueden ser agrupados de manera distinta:

A. Tipo de algoritmo
- Modelos lineales: _PLS_
- Modelos no-lineal: _SVM_, _RF_, _NN_

B. Tipo de reducción de dimensionalidad
- Modelos con selección de variables (_Feature Selection_): son aquellos modelos que reducen dimensionalidad de los datos seleccionando predictores, sin alterar su información; _SVM_, _RF_, _NN_
- Modelos de extracción de variables (_Feature Extraction_): son aquellos modelos que reducen dimensionalidad de los datos mediante métodos de ordenación, es deir transforman la información original de los predictores; _PLS_.

Glosario de términos a usar:
- Hiperparámetro (_hiperparameters_)
- Selección de variables (_variable selection_)
- Importancia de variables (_variable importance_)
- Sobreajuste (_overfitting_)
- Colinealidad (_colineality_)
- Residuos (_residuals_)
- Autocorrelación espacial (_spatial autocorrelation_)

#### 5.2.1. _Partial Least Square_ (_PLS_)

El método _Partial Least Squares_ (_PLS_) crea componentes lineales de las variables predictivas mediante _ordination_. Funciona de manera similar a _PCA_ en su método de reducción, pero utiliza la información de la variable respuesta (_Y_) para maximizar lo mas posible la varianza de los predictores (_X_). La diferencia reside en que, mientras _PCA_ ignora la variable respuesta _Y_ para determinar las combinaciones lineales, PLS busca aquellas que, además de explicar la varianza observada de _X_, predicen _Y_ lo mejor posible. Puede considerarse como una versión supervisada de _PCA_ para predicción. Por lo mismo, en general _PLS_ guarda mejor información para predicción que _PCA_ en pocos componentes.

_PLS_ es utilizado enormemente en estudios con datos de teledetección ya que reduce eficientemente la información con altos grados de colinealidad (i.e., _colineality_; comparten mucha información similar entre si), como por ejemplo las bandas espectrales.  Antes de utilizar _PLS_, los predictores deben ser normalizados, especialmente las variables están en unidades distintas Similar a _PCA_. Si no se hece esto, _PLS_ podría dar mas importancia simplemente a variables con mayor varianza. Por lo tanto, los predictores deben ser adecuadamente preprocesados antes de realizar el _PLS_.

Al igual que mucho métodos lineales, PLS esta sujeto a problemas de sobreajuste (_overfitting_) si muchas variables son incliidas durante el modelamiento.

Para mas información respecto a _PLS_, ver [este video](https://www.youtube.com/watch?v=WKEGhyFx0Dg) con un ejemplo práctico.

**Paquetes y funciones**
La funcion básica de _PLS_ utiliza la función  ``plsr`` del paquete ``pls``.

**Hiperparámetros**:
- ``ncomp``: número de componentes a incluir.

**Importancia de las variables en _caret_**: Como _PLS_ hace componentes lineales de todos los predictores,  la importancia individual de cada variable va a depender del número de componentes que se seleccione. Al igual que en _PCA_, en _PLS_ se pueden obtener los pesos (_weights_) de cada variable en cada componente. Luego, la importancia de cada variable sería una suma ponderada de los pesos de cada variable.

<p align="center">
  <img width="600" src="img/pls.png">
</p>


#### 5.2.2. _Support Vector Machines_ (_SVM_)

Las máquinas de soporte vectorial (_Support Vector Machine, SVM₎ es un algoritmo de clasificación y regresión muy flexible y poderoso. _SVM_ busca la separación (clasificación) o relación (regresión) lineal entre las variables predictivas y las observadas mediante la transformación lineal o no-lineal de los datos a un determinado hiperplano que maximice la linealidad predictiva. Este hiperplano es construido mediante funciones denominadas _kerner_. Existen _kernels_ muy complejos que podrían llevar a los datos a un espacio _N_-dimensional hasta encontrar una relación lineal entre los predictores y los observados.



**Paquetes y funciones**
Existen muchos topod de _SVM_, segun el _kernel_ que se utilice. Algunos de los _kernels_ mas usados son: lineares, polinómicos, y de bases radiales. A modo de ejemplo, vamos a utilizar el _kernel_ _radial basis_ (_RBF_), el cual es uno de los mas utilizados en teledetección. El método ``svmRadial`` de ``caret`` emplea la función ``ksvm()`` del paquete ``kernlab``.

Mas información sobre _SVM_ puede ser vista en esta serie de [videos](https://www.youtube.com/watch?v=PDcXbbCge0E&list=PLC0PzjY99Q_Xc5IK-UE4FX7Loz1auXylY). Ejemplos particulares de _BRF_ se pueden ver en [estos videos](https://www.youtube.com/watch?v=Z2_yh2sice8&list=PLC0PzjY99Q_Xc5IK-UE4FX7Loz1auXylY&index=13).


**Hiperparámetros**:
- ``sigma``: coeficiente del kernel radial.
- ``C``: penalización por violaciones del margen del hiperplano.

<p align="center">
  <img width="600" src="img/svm.png">
</p>


#### 5.2.3. _Random Forest_ (_RF_)

El método de _random forest_ es un algoritmo no-lineal basado en árboles de decisión. Los métodos estadísticos basados en árboles engloban a un conjunto de técnicas supervisadas no paramétricas que consiguen segmentar el espacio de los predictores en regiones simples, dentro de las cuales es más sencillo manejar las interacciones.

_RF_ crea una gran cantidad de árboles de decisión usando separaciones de datos en base a _bagging_, diminutivo de _bootstrap aggregation_, o muestreo repetido (_bootstrapping_; lo vamos a ver mas adelante) con el fin de reducir la varianza de los datos en cada segmento. Cada segmento de datos debe estar lo menos correlacionado con el resto posible, de manera que cada segmento contenga información única. Luego, en cada segmento el algoritmo busca predecir (regresión) o separar las clases (clasificación) de acuerdo a arboles de decisión:

<p align="center">
  <img width="400" src="img/rf1.png">
</p>

Como cada conjunto de datos es aleatorio y levemente distinto, cada árbol puede que resultar en valores distintos. _RF_ luego hace un ensamblaje de todas las predicciones de acurdo a un sistema de votos:

<p align="center">
  <img width="500" src="img/rf2.png">
</p>

La baja correlación entre los modelos es la clave: los modelos no correlacionados pueden producir predicciones emsambladas más precisas que cualquiera de las predicciones individuales. La razón de este maravilloso efecto es que los árboles se protegen unos a otros de sus errores individuales (siempre que no se equivoquen constantemente todos en la misma dirección). Mientras que algunos árboles pueden estar equivocados, muchos otros árboles estarán en lo cierto, por lo que como grupo los árboles son capaces de moverse en la dirección correcta.

**Hiperparámetros**:
- ``mtry``: número de predictores seleccionados en cada iteración.


## 6. Métodos de validación y métricas de ajuste

Antes de crear un modelo predictivo, hay pensar como este modelo se va a validar, y como vamos a medir el nivel de éxito del modelo. Los pasos comunes durante la construcción del modelo son:

- Estimar los parámetros del modelo (i.e., los modelos de entrenamiento)
- Determinar los valores de los parámetros de entrenamiento que no pueden ser directamente (i.e., hiperparametros)
- Calcular el rendimiento del modelo final que predice nuevos datos

¿Cómo usamos los datos para encontrar un modelo óptimo? Normalmente dividimos los datos en un conjunto de datos de entrenamiento y validación (o test):
- Datos de entrenamiento (_train data_): estos datos se utilizan para estimar los parámetros del modelo y para elegir los valores de los hiperparametros el modelo.
- Datos de validación o test (_test data_): estos datos se utilizan para obtener una evaluación independiente de la eficiencia del modelo. No deben utilizar durante el entrenamiento del modelo.

Cuantos más datos usemos, mejores estimaciones obtendremos (siempre que los datos no tengan errores). Dada una cantidad fija de datos,
- demasiados datos de entrenamiento no nos permitirá obtener una buena evaluación de rendimiento predictivo, ya que se utilizan los datos de validación para esto. Podríamos obtener un modelo que se ajuste al entrenamiento datos muy bien, pero no es generalizable a nuevos datos (**sobreajuste**).
- demasiados datos de validación no nos permitirá obtener una buena evaluación de los parámetros del modelo, encontrados con los datos de entrenamiento.


### 6.1. División de datos

Hay unas cuantas formas de hacer la división de los datos: muestreo aleatorio simple, muestreo estratificado basado en el resultado, por fecha, y métodos que se centran en la distribución de los predichos.

La función R ``sample`` puede utilizarse para crear una muestra completamente aleatoria de los datos. El paquete ``caret`` tiene una función ``createDataPartition`` que realiza otras divisiones de datos dentro de grupos de los datos. Una buena división de datos significa:

- Para clasificación, esto significaría un muestreo uniforme dentro de las clases para preservar la distribución del resultado en los conjuntos de entrenamiento y validación
- Para regresión, la función determina los cuantiles de la variable observada y obtiene muestras dentro de esos grupos.

### 6.2. Sobreajuste
Se produce cuando un modelo capta de forma inapropiada las tendencias del conjunto de entrenamiento, por lo que predice de manera errónea en el conjunto de validación. Esto ocurre aunque el modelo en los datos de entrenamiento muestre buenos ajustes.

Por ejemplo, en la siguiente figura se muestra un modelo 1 que esta sobreajustado al encontrar una separación entre la clase azul y la roja. Al ajustar mal los hiperparametros del modelo, se obtuvo una separación entre clases que es demasiado especifica a los datos de entrenamiento (ver círculos aislados). Es altamente probable que ese mismo modelo no se ajuste bien a cualquier otro set de datos.  El modelo 2 por el otro lado, hace una separación mas simple (quizás con ajustes de éxito menores al modelo 1 según los datos de entrenamiento) pero que es mas fácil de utilizar para predecir nuevos datos. Es por esto que la validación se hace **SIEMPRE** con datos que no se utilizan en la construcción del modelo (i.e., el ajuste del modelo 2 es mucho mejor que el del modelo 1 si se usan datos de validación independiente).

<p align="center">
  <img width="700" src="img/overfitting.png">
</p>


### 6.2. Validación simple

Este es el método mas básico, y consiste en separar los datos en un porcentaje para entrenamiento y un porcentaje para validación. Este método debería usarse unicamente cuando se tiene una gran cantidad de observaciones, ya que se asume que la aleatoriedad de la selección es representativa del conjunto de datos, lo cual muchas veces no es cierto. Es por eso que se recomienda usar métodos que incluyan repetición, como los que se presentan a continuación, para incluir en algún momento todas las observaciones en el proceso de entrenamiento y validación.

<p align="center">
  <img width="600" src="img/datasplit.jpg">
</p>

### 6.3. Validación cruzada _K_-fold

Aquí, dividimos aleatoriamente los datos en _K_ distintos bloques de aproximadamente el mismo tamaño.
- Dejamos fuera el primer bloque de datos y ajustamos un modelo.
- Este modelo se utiliza para predecir el bloque de datos de validación (_hold-on_)
- Continuamos este proceso hasta que hayamos predicho en los _K_  bloques de validación

El rendimiento final se basa en las _K_ predicciones de validación. Generalmente se eligen valores de _K_ de 5 o 10. Mientras mas grande el número de _K_, mas bloques de datos se utilizan, y menos datos de validación quedaran en cada bloque. Por ejemplo, si de un total de 12 observaciones se elige un _K_ = 3, significa que las 12 observaciones se van a separar en 3, dando bloques de 4 observaciones (12/3 = 4). Se van a ajustar por lo tanto tres modelos, utilizando dos bloques como entrenamiento (8 datos) y un bloque para validar (4 datos). Ver el ejemplo en a figura abajo. Es este caso particular, un _K_ muy alto podría dejan muy pocos datos para una buena validación. Este método se utiliza para tener una distribución de valores de ajuste (en el ejemplo 3)

Los _K_-fold repetitivos (_repeated K-fold_) crean múltiples versiones de los bloques, y el modelo final será la combinación de los modelos. Por ejemplo, si el modelo de la figura (_K_ = 3) se repite 5 veces, se tendrían al final 3*5 = 15 datos de validación. Recuerden que los bloques son elegidos de forma aleatoria, por lo que cada bloque si sera levemente distinto entre si.

_Leave-one-out_ (LOO) es un caso especial de _K-fold_, donde _K_ = 1. En general es menos robusto que _K_ = 5 o 10, pero puede ser una buena alternative si los datos utilizados tienen pocas observaciones.


<p align="center">
  <img width="600" src="img/kfold.png">
</p>


### 6.4. Separación aleatoria repetitiva

Este método de denomina _leave-group-out_ o _Monte Carlos_, selecciona una determinada porción de los datos para entrenar y validar el modelo (ej., 80/20 %). Esto se repite muchas veces, y el promedio de los modelos se utiliza. La diferencia básica con _K_-fold, es que como cada iteración el completamente aleatoria, hay observaciones que se pueden repetir para entrenar y validar. El número de repeticiones o iteraciones puede ser cualquiera, ya que la partición de los datos no se relaciona a este número. Con muchas repeticiones (ej., 100), el generalmente genera predicciones con menos varianza en cada iteración, pero puede generar un poco mas de sesgo que _K_-fold.

<p align="center">
  <img width="600" src="img/repcv.png">
</p>

### 6.5. _Bootstrapping_

Es básicamente muy parecido a la separación aleatoria repetitiva, con la diferencia de que las selección de observaciones es por el mismo número total de observaciones que se tiene, y esta selección puede ser con remplazo. Por ejemplo, se eligen de manera aleatoria 12 observaciones, de las cuales algunas observaciones serán elegidas mas de una vez y algunas no serán elegidas.

En general, cerca del 63% de las observaciones son utilizadas para entrenamiento y 27% para validar. Este método es particularmente atractivo si se tienen pocas observaciones, ya que se mantiene este número para entrenar modelos de forma constante.


<p align="center">
  <img width="600" src="img/bootstrap.png">
</p>

### 6.6. Validación estratificada y bloqueada

Al utilizar cualquiera de estos tipos de validación, es importante que la durante la separación de los datos en entrenamiento y validación, cada separación sea lo mas representativa de la varianza total de los datos. De esta manera cada modelo entrenado y valores predichos tendrán menos error. ``caret`` se hace cargo de esto, como se explico previamente, integrando segmentación por cuantiles o por clases. Si embargo, hay algunos ejemplos donde se debe hacer algo distinto, llamado validación bloqueada.

Esto se utiliza cuando hay observaciones que están muy correlacionadas entre si, y no queremos que estas observaciones sean aleatoreamente separadas. Por ejemplo, si se tienen mediciones de humedad del suelo a lo largo un un gradiente de humedad de varios años consecutivos (ej., punto1-año1, punto1 1-año2, punto1 1-año3, etc), es muy probables que todas las mediciones hechas en un mismo punto estén altamente correlacionadas. Si usamos algunas de estas observaciones para entrenar un modelo, y algunas para validar, es posible que el ajuste del modelo sea demasiado optimista, simplemente por q nuestros datos de validación se parecen mucho a los entrenamiento. Es este caso, se valida por bloque, y todas pas mediciones del _punto1_ deben ser usadas o para entrenar o validar.

### 6.6. ¿Qué método debo seleccionar?

Esto depende de los datos que se tengan, pero sin duda una de las opciones con repetición. _Leve-one-out_, o _K_-fold con un K = 1, sería la opción mas básica y con menos requerimientos computacionales. Sin embargo, esta puede dar resultados menos robustos. Si se tienen pocos datos (ej., 30-60), en mi opinión bootstrapping es una buena opción, ya que el número total de las observaciones se mantiene durante el entrenamiento. Sin embargo, seria recomendable hacer muchas repeticiones de modelos (ej., 100-500) para obtener una buena distribución de posibles resultados, lo cual podría llevar mas tiempo de procesado.

Por el contrario, si el número de observaciones es grande (ej., mas de 100), K-fold (ojalas con repeticiones) es la mejor opción.
